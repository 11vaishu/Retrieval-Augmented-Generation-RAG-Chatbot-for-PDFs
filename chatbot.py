# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MGacQcozC0XPZ_ThnNVipNAAgEAAYBuE
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q langchain langchain-community faiss-cpu sentence-transformers transformers torch pypdf accelerate bitsandbytes huggingface_hub

# üìÇ Module 1: Imports + Data Preprocessing & Embeddings

from google.colab import files
import torch

# --- LangChain & Community ---
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.llms import HuggingFacePipeline
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate

# --- Hugging Face ---
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

class PDFProcessor:
    def __init__(self):
        self.embeddings = None
        self.vectorstore = None

    def upload_pdf(self):
        """Upload PDF file to Colab"""
        print("üìÅ Please upload your PDF file...")
        uploaded = files.upload()
        if not uploaded:
            print("‚ùå No file uploaded!")
            return None
        pdf_filename = list(uploaded.keys())[0]
        print(f"‚úÖ Uploaded: {pdf_filename}")
        return pdf_filename

    def load_and_process_pdf(self, pdf_path: str):
        """Load and split PDF into chunks"""
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, chunk_overlap=200, length_function=len
        )
        chunks = text_splitter.split_documents(documents)
        print(f"‚úÖ Split into {len(chunks)} chunks")
        return chunks

    def setup_embeddings(self):
        """Initialize embeddings"""
        print("üîß Setting up embeddings...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("‚úÖ Embeddings ready!")

    def create_vector_store(self, chunks):
        """Create FAISS database"""
        print("üóÑÔ∏è Creating vector database...")
        self.vectorstore = FAISS.from_documents(chunks, self.embeddings)
        print("‚úÖ Vector database created!")
        return self.vectorstore

# ü§ñ Module 2: LLM & Chatbot Setup

class QAModel:
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
        self.llm = None
        self.qa_chain = None

    def setup_llm(self):
        """Load lightweight Hugging Face model"""
        print("üß† Loading language model...")
        model_name = "google/flan-t5-base"

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )

        pipe = pipeline(
            "text2text-generation",
            model=model,
            tokenizer=tokenizer,
            max_length=512,
            temperature=0.1,
            repetition_penalty=1.1
        )

        self.llm = HuggingFacePipeline(pipeline=pipe)
        print("‚úÖ Language model loaded!")

    def create_chatbot(self):
        """Build Conversational RAG chatbot"""
        print("üîó Creating Conversational QA chatbot...")
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 4}),
            return_source_documents=True
        )
        print("‚úÖ Chatbot ready!")
        return self.qa_chain

# üöÄ Module 3: Main Runner

def main():
    print("üí¨ Welcome to RAG Chatbot in Google Colab!")
    print("=" * 50)

    # Step 1: Process PDF
    processor = PDFProcessor()
    pdf_path = processor.upload_pdf()   # <-- This will pop up upload dialog
    if not pdf_path:
        return
    chunks = processor.load_and_process_pdf(pdf_path)
    processor.setup_embeddings()
    vectorstore = processor.create_vector_store(chunks)

    # Step 2: Setup chatbot
    qa_model = QAModel(vectorstore)
    qa_model.setup_llm()
    chatbot = qa_model.create_chatbot()

    # Step 3: Chat loop with memory
    chat_history = []
    print("\n" + "=" * 50)
    print("ü§ñ Chatbot ready! Start chatting.")
    print("Type 'quit' to exit.")
    print("=" * 50)

    while True:
        question = input("\nüë© You: ")
        if question.lower() in ["quit", "exit", "q"]:
            print("üëã Goodbye!")
            break

        if question.strip():
            result = chatbot({"question": question, "chat_history": chat_history})
            answer = result["answer"]
            chat_history.append((question, answer))

            print(f"ü§ñ Bot: {answer}")

            # Show sources
            print("\nüìö Sources:")
            for i, doc in enumerate(result["source_documents"]):
                print(f"   {i+1}. {doc.page_content[:200]}...")

if __name__ == "__main__":
    main()